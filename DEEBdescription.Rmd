---
title: "DEEB DB Description"
date: "`r format(Sys.Date())`"
output: 
  html_document:
    toc: yes
    toc_depth: 6
    number_sections: yes
    self_contained: yes
---

```{r setup, include=FALSE}
dbPath <- "."
knitr::opts_chunk$set(echo = FALSE)
```

# Setup

The goal of the *Differential Equation Estimation Benchmark* (DEEB) is to provide simulated data for comparing different methods of learning differential equations from observations.

In this instance of DEEB, we are concerned with ODE models of the following form:
For $f^* \colon \mathbb R^d\to\mathbb R^d$ (from some class of smooth functions) and $u_0^*\in \mathbb R^d$,
let $u^* \colon [0, T] \to \mathbb R^d$ fulfill the ODE
$$
	\frac{\mathrm{d}}{\mathrm{d} t} u^*(t) = f^*(u^*(t))
$$
for $t \in (0, T]$ and $u^*(0) = u_0^*$. For $0 \leq t_1 < \dots < t_n \leq T$, 
we observe $y_i = u^*(t_i) + \varepsilon_i$,
where $\varepsilon_i$ represents observational noise, $\mathbb E[\varepsilon_i] = 0$, and $(\varepsilon_i)_{i = 1,\dots,n}$ are independent.

Using only the observed data $(t_i,y_i)_{i=1,\dots,n}$, we want to infer some information about the model parameters $f$ and $u_0$, e.g., estimating these parameters or predicting $u$ for $t > T$.

The DEEB database consists of simulated data. Using some stochastic model, a function $f$ and an initial state $u^0$ are drawn randomly. Then the ODE is integrated to obtain a trajectory $u$. Finally, a set of observations is drawn from some stochastic noise model.

By drawing $f$ randomly, we necessitate generalization of the estimators performance to a large set of possible true ODEs.

**Extension to multiple trajectories:** For some models we have -- for the same $f$ -- multiple initial states $u_{0,1}^*, u_{0,2}^*, \dots, u_{0,m}^*$, corresponding multiple trajectories $u^*_1(t_i), u^*_2(t_i), \dots, u^*_m(t_i)$, and observations $y_{j,i} = u_j^*(t_i) + \varepsilon_{j,i}$.

# Terminology

* **model**: a distribution from which $(f, u_0)$ and $(\varepsilon_i)_{i=1,\dots,n}$ can be drawn 
* **trajectory**: a function $u \colon [0,T] \to \mathbb R^d$
* **truth**: the ground truth, the true trajectory $u^*$
* **set of observations** (abbreviated as **obs**): $n$ observations $(t_i, y_i)_{i=1,\dots,n}$ for one truth
* **task**: a description of what variables should be estimated from a set of observations
* **estimator**: a method / algorithm for fulfilling a task, i.e., mapping from an obs to the variables described in a task
* **estimation** (abbreviated as **esti**): values for the variables described in a task generated by an estimator


# The Models

Each model comes with 25 truths and each truth comes with 4 obs: 

* obs0001: no noise
* obs0002: low noise
* obs0003: medium noise
* obs0004: high noise

Note that the noise is defined for a model and not for a truth, i.e., *low noise* may be quite a lot of noise for some truths.

The parameter values of the models can be found in the `Opts_Run.json`.

Notation:

* $\mathcal{N}(\mu, \sigma^2)$ the normal distribution with mean $\mu$ and variance $\sigma^2$
* $\mathsf{Unif}(X)$ the uniform distribution on the set $X$
* $B_{d}(m, r)$ a $d$-dimensional ball with radius $r$ and center $m$
* $\Delta t$: the time step for the observations, i.e., $t_i = (i-1)\cdot \Delta t$.

```{r include=FALSE}
getOpts <- function(model) ConfigOpts::readOptsBare(file.path(dbPath, model, "Opts_Run.json"))
```


```{r include=FALSE}
printObsOpts <- function(opts, d) {
  cat(paste0(r"(* $n = )",opts$n,"$\n"))
  cat(paste0(r"(* $\Delta t = )",opts$timeStep,"$\n"))
  cat(paste0(r"(* $\varepsilon_i \sim \mathcal{N}(0, \sigma^2_k)^ )", d, r"($ with $\sigma_k = )",
      paste(opts$scales, collapse = ", "), "$\n"))
}
printU0Sampler <- function(opts, d) {
  switch(
    ConfigOpts::getClassAt(opts, 3),
    uniform = sprintf("* $u_0 \\sim \\mathsf{Unif}([%s, %s]^%d)$", format(opts$range[1]), format(opts$range[2]), d),
    uniformOnBall = sprintf("* $u_0 \\sim \\mathsf{Unif}(B_{%d}(0, %s))$", d, format(opts$range[2])),
    lorenz63 = sprintf("* $u_0 \\sim \\mathsf{Unif}(A)$, where $A$ is the Lorenz attractor"),
    "* unknown"
  ) |> cat("\n")
}
printFunSampler <- function(opts) {
  NULL
}
```


```{r include=FALSE}
printTruthAndObs <- function(model) {
  opts <- getOpts(model)
  d <- opts$truth$deFunSampler$d
  printFunSampler(opts$truth$deFunSampler)
  printU0Sampler(opts$truth$u0Sampler, d)
  printObsOpts(opts$observation, d)
}
```


```{r include=FALSE}
printTasks <- function(model) {
  basePath <- file.path(dbPath, model, "task")
  taskFiles <- dir(basePath, "^task\\d+\\.json$")
  taskNrs <- as.integer(substr(taskFiles, 5, 7))
  cat("Tasks:\n\n")
  for (i in seq_along(taskFiles)) {
    task <- ConfigOpts::readOptsBare(file.path(basePath, taskFiles[i]), FALSE)
    printTask(task, taskNrs[i])
  }
}

printTask <- function(task, nr) {
  cat(sprintf("* %02d (%s): ", nr, task$name))
  descr <- switch(
    ConfigOpts::getClassAt(task, 2),
    estiObsTrajs = sprintf(
      "estimate observed trajectory in time interval [%s, %s] with time step %s",
      format(task$predictionTime[1]), format(task$predictionTime[2]), format(task$timeStep)),
    newTrajs = paste0(
      sprintf(
        "predict new trajectories in the time interval [%s, %s] time step %s starting from given initial points ",
        format(task$predictionTime[1]), format(task$predictionTime[2]), format(task$timeStep)),
      paste(
        apply(task$initialState, 1, \(s) paste0("(",paste(s, collapse=", "),")")),
        collapse = ", ")
      ),
    velocity = paste0(
      "estimate $f$ on a grid with step sizes ", 
      paste(task$gridSteps, collapse=", "), 
      " and ranges ",
      paste(
        apply(task$gridRanges, 1, \(rg) sprintf("[%s, %s]", format(rg[1]), format(rg[2]))),
        collapse = ", ")),
    "unknown")
  cat(descr, "\n")
}
```

## lotkaVolterra

The [Lotka--Volterra](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations) system.

* Coefficients $\sim \mathsf{Unif}([0,2])^4$ 
```{r results='asis'}
printTruthAndObs("lotkaVolterra")
```

```{r results='asis'}
printTasks("lotkaVolterra")
```

## lorenz63std

The [Lorenz63](https://en.wikipedia.org/wiki/Lorenz_system) system with fixed constant parameters $\rho = 28$, $\sigma = 10$, $\beta = \frac83$.

```{r results='asis'}
printTruthAndObs("lorenz63std")
```

```{r results='asis'}
printTasks("lorenz63std")
```

## lorenz63random

The [Lorenz63](https://en.wikipedia.org/wiki/Lorenz_system) system with randomly chosen parameters.

* $\rho \sim \mathcal{N}(28, 2.8^2)$, $\sigma \sim \mathcal{N}(10, 1)$, $\beta \sim \mathcal{N}(8/3, (8/30)^2)$
```{r results='asis'}
printTruthAndObs("lorenz63random")
```

```{r results='asis'}
printTasks("lorenz63random")
```

## polyDeg2Dim2single

A polynomial model in 2 dimensions where $f$ is chosen as a random polynomial of degree 2 in each dimension.

* Coefficients $\sim \mathsf{Unif}([-\frac12,\frac12])$ 
```{r results='asis'}
printTruthAndObs("polyDeg2Dim2single")
```

Trajectories that include values with $\Vert u(t) \Vert > 10$ are rejected.
To make the sampled systems more interesting, a sampled ODE is rejected if the true trajectory has less than 10 "turns". A "turn" is s change in sign of the velocity in any of the 2 dimensions. (TODO: describe more precisely, or change condition)

```{r results='asis'}
printTasks("polyDeg2Dim2single")
```

## polyDeg2Dim2multi

As above, but with $m = 20$ trajectories.

```{r results='asis'}
printTruthAndObs("polyDeg2Dim2multi")
```

Trajectories that include values with $\Vert u(t) \Vert > 10$ are rejected. The "turning"-condition is not active in this model.

```{r results='asis'}
printTasks("polyDeg2Dim2multi")
```

## localConstDim3single

A nonparametric model in 3 dimensions where $f$ is chosen as follows:

* Sample $30$ locations inside a 3D-ball of radius 10 uniformly at random
* For each location sample a 3D vector from $\mathcal N (0,1)^3$
* Create a smooth function $\tilde f$ by local constant regression on these values with bandwidth 2 and the Gaussian density as a kernel.
* $f(x) = \tilde f(x)$ for $\Vert x\Vert \leq 5$. To prevent the system from diverging $\tilde f$ is overlayed with inward pointing vectors for $\Vert x\Vert > 5$: $$f(x) = -(\Vert x\Vert-5) \frac{x}{\Vert x\Vert} + (10-\Vert x\Vert) \tilde f(x)$$

```{r results='asis'}
printTruthAndObs("localConstDim3single")
```

To make the sampled systems more interesting, a sampled ODE is rejected if the true trajectory has less than 15 "turns". A "turn" is s change in sign of the velocity in any of the 3 dimensions. (TODO: describe more precisely, or change condition)

```{r results='asis'}
printTasks("localConstDim3single")
```

## localConstDim3multi

As above, but with $m = 30$ trajectories.

```{r results='asis'}
printTruthAndObs("localConstDim3multi")
```

The "turning"-condition is not active in this model.

```{r results='asis'}
printTasks("localConstDim3multi")
```

# Folder structure

In the database root folder, you find one folder for each model. 
Each model folder contains the following items:

* folder `estimation`: contains a folder for each estimator
* folder `estimation/<estimator>`: contains the estimators outputs: for each truth, obs, and task, one file `truth<NNNN>obs<NNNN>task<NN>esti.csv`
* folder `evaluation`: The HTML files here will display plots and score values of the estimator results for this model.
* folder `example`: An additional set of truths and obs drawn with a different random seed. Can be used for manual tuning.
* folder `observation`: Contains obs in files `truth<NNNN>obs<NNNN>.csv`.
* folder `task`: Contains files `task<NN>.json` describing the tasks.
* folder `truth`: Contains files `obs_truth<NNNN>.csv` and `task<NN>truth<NNNN>.csv` of true trajectories (and vector fields if required by a task) and json files describing the parameters of the true ODE.
* file `Opts_Run.json`: a config file describing the model parameters



# Files and File Formats

Almost all files in the data base are in one of the following formats

* .csv: data (truth, obs, esti)
* .json: configuration parameters
* .html, .png: plots, tables of scores

The format of csv files used to describe trajectories (including observations) is as follows:

## csv for trajectories

The file is a plain text file with values separated by the delimiter `,` and rows separated by a linebreak. The first row contains the column names. Possible column names are `trajId`, `time`, `state<N>`, `deriv<N>`. Column do not have a prescribed order but are identified by their name.

* `trajId` (integer): Used for models with multiple trajectories for the same instance.
* `time` (float): time points $t_i$
* `state<N>`, i.e., `state1`, `state2`, ... (float): dimension `<N>` of the state $u(t_i)$ or some state $u$
* `deriv<N>`, i.e., `deriv1`, `deriv2`, ... (float): dimension `<N>` of the derivative $\dot u(t_i)$ or $f(u)$

At least the columns `time` and `state1`, or `state1` and `deriv1` must exist.

## json for configurations

Typically the files contain at the top level an object with a `"_class"` field. The object can have further fields, including further objects with a `"_class"` field. The different classes have special meaning in the DEEB environment, e.g., every task has a class `["<taskType>", "Task", "Opts"]`. Each class has a prescribed set of fields. Further information can be found in the R-packages associated to DEEB:

* <https://github.com/chroetz/ConfigOpts/tree/main/inst/defaultOpts>
* <https://github.com/chroetz/DEEBdata/tree/main/inst/defaultOpts>
* <https://github.com/chroetz/DEEBeval/tree/main/inst/DefaultOpts>

For testing a new estimator on data provided in a DEEB database, the only json-configuration files that one needs are the tasks:

## json for tasks

Tasks follow the general format of json for configuration. To find out, what is to be estimated, first take a look at the first entry of the field `"_class"`:

* `"estiObsTrajs"`: Estimate the observed trajectory or trajectories. The field `"predictionTime"` shows which time interval is to be estimated and `"timeStep"` grid size for the output times. 
* `"newTrajs"`: Estimate new trajectories starting with the initial states given in the field `"initialState"` on the time interval given in `"predictionTime"` with time grid size given in `"timeStep"`. 
* `"velocity"`: Estimate $f^*(x)$ for $x$ on a grid defined by the fields `"gridRanges"`, `"gridSteps"`.

The information in theses task files is also described in [The Models].




# How to add a new estimator

* choose a model you want to apply the estimator to
* go to `<model>/example`
* take a look at `example/evaluation/showPlots.html` to see how the data looks like
* open `example/task/task<NN>.json` to see what is to be estimated or take a look at the respective subsection of [The Models]
* use the `example/observation/truth<NNNN>obs<NNNN>.csv` and corresponding `example/truth/obs_truth<NNNN>.csv` (observation values without noise) and `example/truth/task<NN>truth<NNNN>.csv` (optimal solution for task) files to tune the estimator
* You can use everything in the example folder for tuning. After that the estimator must be fixed. This means, conceptually it must be a function that takes an obs in the form of a `truth<NNNN>obs<NNNN>.csv` file and a task in the form of a `task<NN>.json` file and produce an estimation in the form of a `truth<NNNN>obs<NNNN>task<NN>esti.csv` file. No side-effects like storing values / adapting parameters between different sets of observations are allowed. Of course the estimator may take one obs, split it into training and validation data, and perform training and hyperparameter optimization. But everything that goes beyond the tuning on the example data has to be done within one application of the estimator to an obs and therefore be repeated for each new obs.
* Produce `truth<NNNN>obs<NNNN>task<NN>esti.csv` files from `<model>/obsevation` (without looking at `<model>/truth`) and put them into `<model>/estimation/<EstimatorName>`.
* run `DEEBeval::interact()` and choose the appropriate options (see below)


# Creating the Evaluation

Make sure that all required packages are installed and up to date. To do that run the R-script `installDEEB.R` via `Rscript installDEEB.R` or run R and call

```{r eval=FALSE, echo=TRUE}
if (!require("remotes", quietly = TRUE)) install.packages("remotes")
if (!require("kableExtra", quietly = TRUE)) install.packages("kableExtra")
if (!require("purrr", quietly = TRUE)) install.packages("purrr")
if (!require("stringr", quietly = TRUE)) install.packages("stringr")
remotes::install_github("chroetz/ConfigOpts")
remotes::install_github("chroetz/DEEBtrajs")
remotes::install_github("chroetz/DEEBpath")
remotes::install_github("chroetz/DEEBplots")
remotes::install_github("chroetz/DEEBdata")
remotes::install_github("chroetz/DEEBeval")
```

Then `cd` to the DEEB database and execute `Rscript -e 'DEEBeval::interact()'` in the terminal. This will start a dialog in which you can choose which parts of the database should be evaluated. If this is run on the PIK cluster, it submits a SLURM job in the end.

After the job has finished the new scores and plots should be ready to view via the respective HTML pages.


# Evaluation Results

There are different scores that can be computed by comparing an estimation to a truth `task<NN>truth<NNNN>.csv` to an estimation `truth<NNNN>obs<NNNN>task<NN>esti.csv`. In `<model>/evaluation/eval_scores.html`, these scores are presented as skill score relative to the optimal score attained by the true values and a reference score. The reference score is "Const": always estimating the mean of the observations. A score of 1 is perfect, i.e., as good as the truth. A score of 0 is bad, i.e., as good as "Const". Negative scores result from estimations that are even worse.

For a visual comparison, open `<model>/evaluation/showPlots.html` and select the items you are interested in to show the respective plots from the folder `<model>/evaluation/plots`.

## Scores

### L2

Can be applied to trajectories and vector fileds. For functions $g, h\colon \mathcal{X} \to \mathbb R^d$ and grid points $(x_i)_{i=1,\dots,m}$

$$ \frac 1m \sum_{i=1}^m \Vert g(x_i) - h(x_i) \Vert$$
where for $v \in \mathbb R^d$, $\Vert v \Vert = \sqrt{\sum_{j=1}^d v_j^2}$.

Approximates the integrated Euclidean distance
$$ \frac 1{|\mathcal X|} \int_{\mathcal X} \Vert g(x) - h(x) \Vert \,\mathrm d x$$

### WarpedL2

Dynamic time warping distance. For trajectories $g, h\colon [0,T]\to\mathbb R^d$ and a grid $(t_i)_{i=1,\dots,m}$.
$$ \frac 1m \sum_{i=1}^m \Vert g(\varphi_g(t_i)) - h(\varphi_h(t_i)) \Vert$$
where $\varphi_g, \varphi_h\colon[0,T]\to[0,T]$ are continuous monotonically increasing functions that minimize the expression above. Calculated via the R-package `dtw`.

### W2 

The 2-Wasserstein distance, also called Earth Mover's Distance. This is a distance on the set of states $u = (u_i)_{i=1,\dots,m}, v = (v_i)_{i=1,\dots,m} \subset \mathbb R^d$, of a trajectory that does not depend on the corresponding times.

$$W_2 (u, v)^2 = \inf_\tau \sum_{i=1}^m \Vert u_i - v_{\tau(i)}\Vert^2$$
where the infimum is taken over all permutations of $\{1, \dots, m\}$. This distance is calculated using the R-package `transport`.

### FollowTime

For a *follower* $\hat u \colon [0, T] \to \mathbb R^d$ and a *target* $u^* \colon [0, T] \to \mathbb R^d$ we calculate the *follow time* $t_{\mathsf{follow}} \in [0, T]$ that is maximal such that $\hat u$ is *close* to $u^*$ on $[0, t_{\mathsf{follow}}]$.

To define *closeness*, we calculate the distance between points of dimension $d+1$ with the time as an additional dimension: The *follower* is *close* to the *target* at time $t_0 \in [0, T]$ if 
$$\inf_{t\in[0,T]} \left\Vert\begin{pmatrix}\hat u_{\mathsf{normal}}(t_0)-  u_\mathsf{normal}^*(t)\\\alpha (t_0 - t)\end{pmatrix}\right\Vert \leq r\,,$$
where $\hat u_\mathsf{normal}$ and $u^*_\mathsf{normal}$ are 
$\hat u$ and $u^*$ after a linear transformation that makes the empirical covariance matrix of $u^*$ the identity matrix times $1/\sqrt d$.

The parameters $\alpha,r > 0$ have to be chosen suitably:

* $\alpha$ sets the relation between space- and time-distance (we use $\alpha=1$).
* $r$ is the threshold for *closeness* (we use $r=0.5$)

## Trimmed Score

The skill scores (trimmed to $[0,1]$) are averaged over the truth numbers to gain an aggregated score in a separate table. 

## Rank Score

Aside from the skill scores, `<model>/evaluation/eval_scores.html` also shows the rank of the estimator's skill score for a given (task, truth, obs, score)-combination. Furthermore, in a separate table, the ranks are aggregated over the truth numbers for each (task, obs, score)-combination into a rank score. This is the median over all truths of the normed ranks: For truth nr $j$ and method $m$ with rank $r_{j, m} \in \mathbb N$:
$$1 -\frac{r_{j, m} - \min_{\tilde m} r_{j, \tilde m}}{\max_{\tilde m} r_{j, \tilde m} - \min_{\tilde m} r_{j, \tilde m}}$$


## Plots

### stateSpace

Image of trajectories.
Truth in red. Estimation in turquoise.
If $d>2$, a projection to 2 dimensions is calculated by a principle component analysis on the truth.
If method and task `-` are chosen: Observations as black dots.

### timeState

Time vs state for each dimension.
Truth in red. Estimation in turquoise. If method and task `-` are chosen: Observations as black dots.

### timeWarp

Time vs state after time warp for each dimension.
Truth in red. Estimation in turquoise.

### followTime

Closeness of follower to truth in black. Threshold for closeness in red. Follow time in blue.

### vectorFieldDiff 

vector field of difference $\hat f - f^*$ (projection to 2D applied if necessary)

### vectorFieldEsti 

estimated vector field $\hat f$ (projection to 2D applied if necessary)

### vectorField 

true vector field $f^*$ (projection to 2D applied if necessary)